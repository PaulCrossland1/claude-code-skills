---
name: 003-execute-tasks
description: Generate a task-loop.sh script to execute tasks from .claude/tasks.json autonomously using codex or claude CLI. Use when the user wants to run project tasks automatically.
---

# Task Loop Generator

Generate a `task-loop.sh` script that autonomously executes tasks from `.claude/tasks.json`. The script loops through pending tasks, runs the selected AI agent (codex/claude), verifies success criteria, and updates project state files.

---

## When to Use

Use this skill when the user says any of:
- "generate task loop"
- "create task runner"
- "run tasks automatically"
- "execute project tasks"
- "start the task loop"
- "run /003" or "/003-execute-tasks"

---

## Prerequisites

Before generating the script, verify:

1. **Project setup exists**: `.claude/tasks.json` must exist (created by `/002-setup-project`)
2. **Agent CLI installed**: User needs either:
   - `codex` — `npm i -g @openai/codex`
   - `claude` — `npm i -g @anthropic-ai/claude-code`

Check prerequisites:
```bash
# Check for tasks.json
ls -la .claude/tasks.json

# Check for agent CLIs
which codex && echo "codex installed"
which claude && echo "claude installed"
```

If `.claude/tasks.json` doesn't exist:
```
No tasks.json found. Run /002-setup-project first to generate the project structure and tasks.
```

---

## Step 1: Agent Selection

Ask the user which agent to use as the default:

```json
{
  "questions": [{
    "question": "Which AI agent should run the tasks?",
    "header": "Agent",
    "options": [
      {"label": "Codex (Recommended)", "description": "OpenAI Codex CLI — fast, good for most tasks"},
      {"label": "Claude", "description": "Anthropic Claude Code CLI — thorough, better for complex tasks"}
    ],
    "multiSelect": false
  }]
}
```

---

## Step 2: Generate task-loop.sh

Write the `task-loop.sh` script to the project root. Use the template below, substituting the selected agent as the default.

**Important**: Write the script exactly as shown — it's been tested and works correctly.

### Script Template

```bash
#!/bin/bash
# Task Loop — Execute tasks sequentially using codex or claude
# Generated by /003-execute-tasks skill
#
# Usage:
#   ./task-loop.sh                    # Run with default agent
#   ./task-loop.sh --agent codex      # Run with codex
#   ./task-loop.sh --agent claude     # Run with claude
#   ./task-loop.sh --max 10           # Max 10 iterations
#   ./task-loop.sh --dry-run          # Show prompts without executing
#   ./task-loop.sh --verify-only      # Only verify criteria, mark complete if passing
#   ./task-loop.sh --no-monitor       # Disable HTML monitor (for CI/headless)

set -euo pipefail

# ============================================================================
# Configuration
# ============================================================================

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
TASKS_FILE="$ROOT_DIR/.claude/tasks.json"
CONTEXT_FILE="$ROOT_DIR/.claude/CONTEXT.md"
ARCHITECTURE_FILE="$ROOT_DIR/.claude/ARCHITECTURE.md"
PRD_FILE="$ROOT_DIR/.claude/PRD.md"
PROGRESS_FILE="$ROOT_DIR/.claude/PROGRESS-NOTES.md"
PROMPTS_DIR="$ROOT_DIR/.claude/prompts"
RUNS_DIR="$ROOT_DIR/.claude/runs"

# Defaults (AGENT_DEFAULT will be substituted based on user selection)
AGENT="${AGENT_DEFAULT}"
MAX_ITERATIONS=100
DRY_RUN=false
VERIFY_ONLY=false
NO_MONITOR=false

# Ralph-style retry configuration
MAX_TASK_RETRIES=3

# Retry state directory (bash 3.x compatible - no associative arrays)
RETRY_STATE_DIR="$ROOT_DIR/.claude/.retry-state"
mkdir -p "$RETRY_STATE_DIR"

# Helper functions for retry state (bash 3.x compatible)
get_task_attempts() {
  local tid="$1"
  local file="$RETRY_STATE_DIR/${tid}.attempts"
  [[ -f "$file" ]] && cat "$file" || echo "0"
}

set_task_attempts() {
  local tid="$1" val="$2"
  echo "$val" > "$RETRY_STATE_DIR/${tid}.attempts"
}

get_task_last_log() {
  local tid="$1"
  local file="$RETRY_STATE_DIR/${tid}.log"
  [[ -f "$file" ]] && cat "$file" || echo ""
}

set_task_last_log() {
  local tid="$1" val="$2"
  echo "$val" > "$RETRY_STATE_DIR/${tid}.log"
}

get_task_failed_criteria() {
  local tid="$1"
  local file="$RETRY_STATE_DIR/${tid}.criteria"
  [[ -f "$file" ]] && cat "$file" || echo ""
}

set_task_failed_criteria() {
  local tid="$1" val="$2"
  echo "$val" > "$RETRY_STATE_DIR/${tid}.criteria"
}

clear_task_retry_state() {
  local tid="$1"
  rm -f "$RETRY_STATE_DIR/${tid}.attempts" \
        "$RETRY_STATE_DIR/${tid}.log" \
        "$RETRY_STATE_DIR/${tid}.criteria"
}

# Monitor configuration
MONITOR_PORT=8111
MONITOR_PID=""

# ============================================================================
# Parse Arguments
# ============================================================================

while [[ $# -gt 0 ]]; do
  case "$1" in
    --agent)
      AGENT="$2"
      shift 2
      ;;
    --max)
      MAX_ITERATIONS="$2"
      shift 2
      ;;
    --dry-run)
      DRY_RUN=true
      shift
      ;;
    --verify-only)
      VERIFY_ONLY=true
      shift
      ;;
    --no-monitor)
      NO_MONITOR=true
      shift
      ;;
    *)
      echo "Unknown argument: $1"
      echo "Usage: ./task-loop.sh [--agent codex|claude] [--max N] [--dry-run] [--verify-only] [--no-monitor]"
      exit 1
      ;;
  esac
done

# ============================================================================
# Agent Commands
# ============================================================================

get_agent_cmd() {
  case "$1" in
    codex)
      echo "codex exec --yolo --skip-git-repo-check -"
      ;;
    claude)
      echo "claude -p --dangerously-skip-permissions"
      ;;
    *)
      echo "codex exec --yolo --skip-git-repo-check -"
      ;;
  esac
}

AGENT_CMD="$(get_agent_cmd "$AGENT")"

# ============================================================================
# Monitor Functions
# ============================================================================

# Copy monitor template from skill directory
copy_monitor_template() {
  local template="$HOME/.claude/skills/003-execute-tasks/monitor.html"
  local target="$ROOT_DIR/.claude/monitor.html"
  if [[ -f "$template" ]]; then
    cp "$template" "$target"
  fi
}

# Start local HTTP server for monitor
start_monitor_server() {
  # Check if port is already in use
  if lsof -i ":$MONITOR_PORT" &>/dev/null; then
    echo "Monitor server already running on port $MONITOR_PORT"
    return
  fi

  cd "$ROOT_DIR/.claude"
  python3 -m http.server "$MONITOR_PORT" &>/dev/null &
  MONITOR_PID=$!
  cd "$ROOT_DIR"

  echo ""
  echo "Monitor: http://localhost:$MONITOR_PORT/monitor.html"

  # Open in default browser
  if command -v open &>/dev/null; then
    open "http://localhost:$MONITOR_PORT/monitor.html" 2>/dev/null || true
  elif command -v xdg-open &>/dev/null; then
    xdg-open "http://localhost:$MONITOR_PORT/monitor.html" 2>/dev/null || true
  fi
}

# Generate state.json for monitor polling
update_state_json() {
  local status="$1"
  local current_task_json="$2"
  local last_completed_json="${3:-}"

  python3 - "$TASKS_FILE" "$ROOT_DIR/.claude/state.json" "$AGENT" "$status" "$current_task_json" "$last_completed_json" "$RUN_TAG" <<'PY'
import json, sys, os
from pathlib import Path
from datetime import datetime, timezone

tasks_file, state_file, agent, status = Path(sys.argv[1]), Path(sys.argv[2]), sys.argv[3], sys.argv[4]
current_task_json = sys.argv[5] if len(sys.argv) > 5 and sys.argv[5] else None
last_completed_json = sys.argv[6] if len(sys.argv) > 6 and sys.argv[6] else None
run_tag = sys.argv[7] if len(sys.argv) > 7 else ""

# Load tasks
data = json.loads(tasks_file.read_text()) if tasks_file.exists() else {}
tasks = data.get("tasks", [])
project_name = data.get("project", {}).get("name", "Unknown Project")

# Count by status
def get_status(t):
    s = str(t.get("status") or "pending").strip().lower()
    if s in ("in_progress", "in-progress"): return "in_progress"
    return s

completed = [t["id"] for t in tasks if get_status(t) == "completed"]
in_progress = [t["id"] for t in tasks if get_status(t) == "in_progress"]
pending = [t["id"] for t in tasks if get_status(t) == "pending"]
blocked = [t["id"] for t in tasks if get_status(t) == "blocked"]

# Parse current task
current_task = None
if current_task_json:
    try:
        ct = json.loads(current_task_json)
        started = ct.get("started_at")
        elapsed = 0
        if started:
            try:
                start_dt = datetime.fromisoformat(started.replace("Z", "+00:00"))
                elapsed = int((datetime.now(timezone.utc) - start_dt).total_seconds())
            except: pass
        current_task = {
            "id": ct.get("id", ""),
            "name": ct.get("name", ""),
            "phase": ct.get("phase", "other"),
            "started_at": started,
            "elapsed_seconds": elapsed,
            "attempt": ct.get("attempt", 1)
        }
    except: pass

# Parse last completed
last_completed = None
if last_completed_json:
    try:
        lc = json.loads(last_completed_json)
        duration = 0
        if lc.get("started_at") and lc.get("completed_at"):
            try:
                s = datetime.fromisoformat(lc["started_at"].replace("Z", "+00:00"))
                e = datetime.fromisoformat(lc["completed_at"].replace("Z", "+00:00"))
                duration = int((e - s).total_seconds())
            except: pass
        last_completed = {
            "id": lc.get("id", ""),
            "name": lc.get("name", ""),
            "duration_seconds": duration
        }
    except: pass

# Find recent log file
recent_log = None
runs_dir = state_file.parent / "runs"
if runs_dir.exists() and run_tag:
    logs = sorted(runs_dir.glob(f"run-{run_tag}-*.log"), key=lambda p: p.stat().st_mtime, reverse=True)
    if logs:
        recent_log = logs[0].name

# Build state
state = {
    "project": project_name,
    "timestamp": datetime.now(timezone.utc).isoformat(),
    "iteration": data.get("project", {}).get("completed_tasks", 0) + 1,
    "agent": agent,
    "status": status,
    "current_task": current_task,
    "progress": {
        "completed": len(completed),
        "total": len(tasks),
        "percent": int(len(completed) / len(tasks) * 100) if tasks else 0
    },
    "tasks_by_status": {
        "completed": completed,
        "in_progress": in_progress,
        "pending": pending,
        "blocked": blocked
    },
    "recent_log": recent_log,
    "loop_log": "loop.log",
    "last_completed": last_completed
}

state_file.write_text(json.dumps(state, indent=2) + "\n")
PY
}

# Cleanup function
cleanup() {
  if [[ -n "$MONITOR_PID" ]]; then
    kill "$MONITOR_PID" 2>/dev/null || true
  fi
  # Clean up retry state directory
  rm -rf "$RETRY_STATE_DIR" 2>/dev/null || true
}
trap cleanup EXIT

# ============================================================================
# Check Prerequisites
# ============================================================================

check_agent() {
  local agent_bin="${AGENT_CMD%% *}"
  if ! command -v "$agent_bin" >/dev/null 2>&1; then
    echo "Agent not found: $agent_bin"
    case "$AGENT" in
      codex) echo "Install: npm i -g @openai/codex" ;;
      claude) echo "Install: npm i -g @anthropic-ai/claude-code" ;;
    esac
    exit 1
  fi
}

[[ "$DRY_RUN" != "true" ]] && [[ "$VERIFY_ONLY" != "true" ]] && check_agent

if [[ ! -f "$TASKS_FILE" ]]; then
  echo "Tasks file not found: $TASKS_FILE"
  echo "Run /002-setup-project first to generate tasks."
  exit 1
fi

mkdir -p "$PROMPTS_DIR" "$RUNS_DIR"

# ============================================================================
# Start Monitor (unless disabled)
# ============================================================================

LOOP_LOG="$ROOT_DIR/.claude/loop.log"

if [[ "$NO_MONITOR" != "true" ]]; then
  copy_monitor_template
  start_monitor_server

  # Capture all stdout/stderr to loop.log while still showing in terminal
  exec > >(tee -a "$LOOP_LOG") 2>&1
  # Truncate log at start of each run
  : > "$LOOP_LOG"
fi

# ============================================================================
# Task Selection
# ============================================================================

select_next_task() {
  python3 - "$TASKS_FILE" <<'PY'
import json, sys
from pathlib import Path

tasks = json.loads(Path(sys.argv[1]).read_text()).get("tasks", [])

def status(t): return str(t.get("status") or "pending").strip().lower()
def done(tid): return any(t.get("id") == tid and status(t) == "completed" for t in tasks)

candidate = None
for t in tasks:
    if status(t) != "pending": continue
    deps = t.get("depends_on") or []
    deps = [deps] if not isinstance(deps, list) else deps
    if all(done(d) for d in deps):
        candidate = t
        break

remaining = sum(1 for t in tasks if status(t) != "completed")
print(json.dumps({"total": len(tasks), "completed": len(tasks) - remaining, "remaining": remaining, "task": candidate}))
PY
}

# ============================================================================
# Verify Success Criteria
# ============================================================================

verify_success_criteria() {
  python3 - "$1" "$ROOT_DIR" <<'PY'
import json, subprocess, sys, re, os, urllib.request, urllib.error
from pathlib import Path

task, root = json.loads(sys.argv[1]), Path(sys.argv[2])
criteria = task.get("success_criteria", [])
if not criteria: print("PASS: No criteria"); sys.exit(0)

all_ok = True
for c in criteria:
    t, ok = c.get("type", ""), False
    try:
        if t == "file_exists":
            ok = (root / c.get("target", "")).exists()
            print(f"{'✓' if ok else '✗'} file_exists: {c.get('target')}")
        elif t == "file_contains":
            p = root / c.get("target", "")
            ok = p.exists() and bool(re.search(c.get("pattern", ""), p.read_text()))
            print(f"{'✓' if ok else '✗'} file_contains: {c.get('target')} ~ /{c.get('pattern')}/")
        elif t in ("command_succeeds", "type_checks", "test_passes", "lint_passes"):
            cmd = c.get("command", "npx tsc --noEmit" if t == "type_checks" else "")
            ok = subprocess.run(cmd, shell=True, cwd=str(root), capture_output=True, timeout=300).returncode == 0
            print(f"{'✓' if ok else '✗'} {t}: {cmd}")
        elif t == "server_responds":
            url = c.get("url", "")
            method = c.get("method", "GET")
            expected = c.get("expected_status", 200)
            try:
                req = urllib.request.Request(url, method=method)
                resp = urllib.request.urlopen(req, timeout=10)
                ok = resp.status == expected
            except urllib.error.HTTPError as e:
                ok = e.code == expected
            except Exception:
                ok = False
            print(f"{'✓' if ok else '✗'} server_responds: {method} {url} (expected {expected})")
        elif t == "env_var_set":
            var = c.get("variable", "")
            ok = var in os.environ and bool(os.environ[var])
            print(f"{'✓' if ok else '✗'} env_var_set: {var}")
        elif t == "manual_verify":
            print(f"⚠ manual_verify: {c.get('instruction', 'Manual verification required')}")
            print("  → Skipping (requires human confirmation)")
            ok = True  # Don't fail on manual checks, just warn
        else:
            ok = True
    except Exception as e:
        print(f"✗ {t}: {e}")
    if not ok: all_ok = False

print("\nPASS: All criteria met" if all_ok else "\nFAIL: Some criteria not met")
sys.exit(0 if all_ok else 1)
PY
}

# ============================================================================
# Update Task Status
# ============================================================================

update_task_status() {
  python3 - "$TASKS_FILE" "$1" "$2" "${3:-}" <<'PY'
import json, sys
from pathlib import Path
from datetime import datetime, timezone

p, tid, st, notes = Path(sys.argv[1]), sys.argv[2], sys.argv[3], sys.argv[4] if len(sys.argv) > 4 else ""
data = json.loads(p.read_text())
now = datetime.now(timezone.utc).isoformat()

for t in data.get("tasks", []):
    if t.get("id") == tid:
        t["status"] = st
        if st == "in_progress": t["started_at"] = now
        elif st == "completed":
            t["completed_at"] = now
            data["project"]["completed_tasks"] = data["project"].get("completed_tasks", 0) + 1
        if notes: t["notes"] = notes
        break

p.write_text(json.dumps(data, indent=2) + "\n")
PY
}

# ============================================================================
# Update CONTEXT.md
# ⚠️  IMPORTANT: The regex patterns below depend on section headings in
#     CONTEXT.md matching the template in 002-setup-project/references/
#     document-templates.md. If that template changes, update patterns here.
# ============================================================================

update_context() {
  python3 - "$CONTEXT_FILE" "$TASKS_FILE" "$1" "$2" "$3" "$4" <<'PY'
import sys, re, json
from pathlib import Path
from datetime import datetime

ctx, tasks_p = Path(sys.argv[1]), Path(sys.argv[2])
tid, tname, done, total = sys.argv[3], sys.argv[4], int(sys.argv[5]), int(sys.argv[6])
if not ctx.exists(): sys.exit(0)

tasks = json.loads(tasks_p.read_text()).get("tasks", []) if tasks_p.exists() else []
def st(t): return str(t.get("status") or "pending").strip().lower()

completed = [(t["id"], t["name"]) for t in tasks if st(t) == "completed"]
in_prog = [(t["id"], t["name"]) for t in tasks if st(t) == "in_progress"]
pending = [(t["id"], t["name"]) for t in tasks if st(t) == "pending"]

def deps_met(t):
    deps = t.get("depends_on") or []
    deps = [deps] if not isinstance(deps, list) else deps
    return all(d in [c[0] for c in completed] for d in deps)

next_up = [(t["id"], t["name"]) for t in tasks if st(t) == "pending" and deps_met(t)][:3]

content = ctx.read_text()
now = datetime.now().strftime("%Y-%m-%d %H:%M")
content = re.sub(r'\*\*Last Updated\*\*:.*', f'**Last Updated**: {now}', content)
content = re.sub(r'\*\*Current Task\*\*:.*', f'**Current Task**: [{tid}] {tname} (just completed)', content)
content = re.sub(r'\*\*Progress\*\*:.*', f'**Progress**: {done}/{total} tasks completed', content)

recent = completed[-5:] if len(completed) > 5 else completed
clines = "\n".join(f"- [x] {i}: {n}" for i, n in recent)
if len(completed) > 5: clines = f"- [x] ...and {len(completed)-5} earlier tasks\n" + clines
ilines = "\n".join(f"- [ ] {i}: {n}" for i, n in in_prog) or "- (none)"

phases = {}
for t in tasks:
    if st(t) == "pending":
        ph = t.get("phase", "other")
        phases.setdefault(ph, []).append(t["id"])
plines = "\n".join(f"- [ ] {ph.replace('-',' ').title()} ({', '.join(ids[:3])}{'...' if len(ids)>3 else ''})" for ph, ids in phases.items()) or "- (none)"

built = f"## What's Been Built\n\n### Completed Tasks ({len(completed)})\n{clines}\n\n### In Progress\n{ilines}\n\n### Not Started ({len(pending)})\n{plines}"
content = re.sub(r'## What\'s Been Built.*?(?=\n## [A-Z])', built + "\n\n", content, flags=re.DOTALL)

nlines = "\n".join(f"{i+1}. Run {tid}: {tn}" for i, (tid, tn) in enumerate(next_up)) or ("All tasks completed!" if done == total else "Waiting for blocked tasks.")
nxt = f"## Next Steps\n\n{nlines}\n\n---"
content = re.sub(r'## Next Steps.*?---', nxt, content, flags=re.DOTALL)

ctx.write_text(content)
PY
}

# ============================================================================
# Generate Task Prompt
# ============================================================================

generate_prompt() {
  python3 - "$1" "$2" "$CONTEXT_FILE" "$ARCHITECTURE_FILE" "$ROOT_DIR" <<'PY'
import json, sys
from pathlib import Path

task = json.loads(sys.argv[1])
prompt_file, ctx_file, arch_file, root = Path(sys.argv[2]), Path(sys.argv[3]), Path(sys.argv[4]), Path(sys.argv[5])

tid, tname, tdesc = task.get("id", ""), task.get("name", ""), task.get("description", "")
ctx_files, out_files = task.get("context_files", []), task.get("output_files", [])
criteria = task.get("success_criteria", [])
best_practices = task.get("best_practices", [])

ctx_content = ctx_file.read_text()[:2500] if ctx_file.exists() else ""
arch_content = arch_file.read_text()[:3000] if arch_file.exists() else ""

crit_text = ""
for c in criteria:
    t = c.get("type", "")
    if t == "file_exists": crit_text += f"- File exists: {c.get('target')}\n"
    elif t == "file_contains": crit_text += f"- File {c.get('target')} contains: {c.get('pattern')}\n"
    elif t == "command_succeeds": crit_text += f"- Command succeeds: {c.get('command')}\n"
    else: crit_text += f"- {c.get('description', t)}\n"

bp_text = "\n".join(f"- {bp}" for bp in best_practices) if best_practices else ""

prompt = f"""# Task: {tid} - {tname}

You are an autonomous coding agent. Complete this task fully and correctly.

## Task Details

**ID**: {tid}
**Name**: {tname}

**Description**:
{tdesc}

## Files to Create/Modify

{chr(10).join(f"- `{f}`" for f in out_files) if out_files else "(see description)"}

## Success Criteria

Your work will be verified by these automated checks:

{crit_text or "- Task completed as described"}
{"## Best Practices" + chr(10) + chr(10) + "Follow these patterns:" + chr(10) + bp_text if bp_text else ""}

## Context Files

Read these files first:
{chr(10).join(f"- `{f}`" for f in ctx_files) if ctx_files else "- `.claude/CONTEXT.md`"}

## Current Project State

<context>
{ctx_content or "(no context)"}
</context>

## Architecture Reference

<architecture>
{arch_content or "(no architecture)"}
</architecture>

## Instructions

### Phase 1: Context Gathering (REQUIRED FIRST)

Before writing ANY code, you MUST read and understand:

1. **Read `.claude/CONTEXT.md`** — Current project state
2. **Read `.claude/ARCHITECTURE.md`** — Technical design and patterns
3. **Read all files in "Context Files" section above** — Task-specific context
4. **Read any existing files you will modify** — Understand current implementation

Do NOT skip this step. Understanding the codebase first prevents errors and ensures consistency.

### Phase 2: Implementation

1. Implement what's described in the task
2. Follow patterns established in existing code
3. Ensure all success criteria will pass

### Phase 3: Update State Files (BEFORE finishing)

### Required Updates

**`.claude/tasks.json`** — Set this task's status to "completed", add "completed_at" timestamp

**`.claude/CONTEXT.md`** — Update:
- `**Last Updated**:` to current date/time
- `**Current Task**:` to `[{tid}] {tname} (just completed)`
- `**Progress**:` increment completed count
- `## What's Been Built` section
- `## Next Steps` with next 3 pending tasks

**`.claude/PROGRESS-NOTES.md`** — Append:
```
---

### [{tid}] {tname}

**Status**: Completed ✓
**Completed**: <datetime>
**Summary**: <1-2 sentences>

```

**`.claude/BLOCKERS.md`** — If you hit blockers, document them

**`.claude/DECISIONS.md`** — If you made architectural decisions, add an ADR

## Begin

**Start with Phase 1**: Read CONTEXT.md, ARCHITECTURE.md, and all context files FIRST.
Then implement (Phase 2), then update state files (Phase 3).
"""

prompt_file.write_text(prompt)
print(f"Prompt: {prompt_file}")
PY
}

# ============================================================================
# Generate Prompt WITH Failure Context (Ralph-style retry)
# ============================================================================

generate_prompt_with_failure() {
  local task_json="$1"
  local prompt_file="$2"
  local previous_log="$3"
  local attempt_num="$4"
  local failed_criteria="$5"

  python3 - "$task_json" "$prompt_file" "$CONTEXT_FILE" "$ARCHITECTURE_FILE" "$ROOT_DIR" "$previous_log" "$attempt_num" "$failed_criteria" <<'PY'
import json, sys
from pathlib import Path

task = json.loads(sys.argv[1])
prompt_file, ctx_file, arch_file, root = Path(sys.argv[2]), Path(sys.argv[3]), Path(sys.argv[4]), Path(sys.argv[5])
previous_log_path = sys.argv[6] if len(sys.argv) > 6 else ""
attempt_num = sys.argv[7] if len(sys.argv) > 7 else "1"
failed_criteria = sys.argv[8] if len(sys.argv) > 8 else ""

tid, tname, tdesc = task.get("id", ""), task.get("name", ""), task.get("description", "")
ctx_files, out_files = task.get("context_files", []), task.get("output_files", [])
criteria = task.get("success_criteria", [])
best_practices = task.get("best_practices", [])

ctx_content = ctx_file.read_text()[:2500] if ctx_file.exists() else ""
arch_content = arch_file.read_text()[:3000] if arch_file.exists() else ""

# Read previous log (last 100 lines)
prev_log_content = ""
if previous_log_path and Path(previous_log_path).exists():
    lines = Path(previous_log_path).read_text().split('\n')[-100:]
    prev_log_content = '\n'.join(lines)

crit_text = ""
for c in criteria:
    t = c.get("type", "")
    if t == "file_exists": crit_text += f"- File exists: {c.get('target')}\n"
    elif t == "file_contains": crit_text += f"- File {c.get('target')} contains: {c.get('pattern')}\n"
    elif t == "command_succeeds": crit_text += f"- Command succeeds: {c.get('command')}\n"
    else: crit_text += f"- {c.get('description', t)}\n"

bp_text = "\n".join(f"- {bp}" for bp in best_practices) if best_practices else ""

# Ralph-style failure context section
failure_section = f"""
## ⚠️ PREVIOUS ATTEMPT FAILED (Attempt {attempt_num})

This is a RETRY. Your previous attempt did not pass verification.

### Failed Criteria
{failed_criteria or "Verification failed - check criteria below"}

### What Went Wrong
Review the previous attempt log below and identify:
1. What was attempted
2. Why it failed
3. What needs to be different this time

### Previous Attempt Log (last 100 lines)
```
{prev_log_content or "(no log available)"}
```

### Instructions for This Retry
- DO NOT repeat the same mistakes
- Read the error messages carefully
- Ensure ALL success criteria will pass
- If a file was partially created, check its current state first
"""

prompt = f"""# Task: {tid} - {tname}

You are an autonomous coding agent. Complete this task fully and correctly.
{failure_section}
## Task Details

**ID**: {tid}
**Name**: {tname}
**Attempt**: {attempt_num}

**Description**:
{tdesc}

## Files to Create/Modify

{chr(10).join(f"- `{f}`" for f in out_files) if out_files else "(see description)"}

## Success Criteria

Your work will be verified by these automated checks. ALL must pass:

{crit_text or "- Task completed as described"}
{"## Best Practices" + chr(10) + chr(10) + "Follow these patterns:" + chr(10) + bp_text if bp_text else ""}

## Context Files

Read these files first:
{chr(10).join(f"- `{f}`" for f in ctx_files) if ctx_files else "- `.claude/CONTEXT.md`"}

## Current Project State

<context>
{ctx_content or "(no context)"}
</context>

## Architecture Reference

<architecture>
{arch_content or "(no architecture)"}
</architecture>

## Instructions

### Phase 1: Analyze Previous Failure
1. Read the "Previous Attempt Log" above carefully
2. Identify what went wrong
3. Plan how to fix it

### Phase 2: Context Gathering
1. Read `.claude/CONTEXT.md` — Current project state
2. Read any files that were created/modified in the previous attempt
3. Check what already exists vs what needs to be created

### Phase 3: Implementation (Fix the issues)
1. Address the specific failures from the previous attempt
2. Ensure all success criteria will pass this time
3. Test your changes mentally before finishing

### Phase 4: Update State Files
- Update `.claude/tasks.json` status to "completed"
- Update `.claude/CONTEXT.md`
- Append to `.claude/PROGRESS-NOTES.md`

## Begin

**This is attempt {attempt_num}. Make it count.**
"""

prompt_file.write_text(prompt)
print(f"Retry prompt: {prompt_file} (attempt {attempt_num})")
PY
}

# ============================================================================
# Append to BLOCKERS.md
# ============================================================================

append_to_blockers() {
  local task_id="$1"
  local task_name="$2"
  local failure_reason="$3"
  local attempts="$4"
  local blockers_file="$ROOT_DIR/.claude/BLOCKERS.md"

  cat >> "$blockers_file" <<EOF

---

## [$task_id] $task_name

**Status**: Blocked after $attempts failed attempts
**Blocked At**: $(date '+%Y-%m-%d %H:%M:%S')
**Agent**: $AGENT

### Failure Reason
$failure_reason

### Action Required
- Review the task logs in \`.claude/runs/\`
- Manually fix the issue or adjust the task definition
- Reset status to "pending" in tasks.json to retry

EOF
  echo "Blocker documented in BLOCKERS.md"
}

# ============================================================================
# Run Agent
# ============================================================================

run_agent() {
  local prompt_file="$1" log_file="$2"
  echo "Running: $AGENT"
  echo "Log: $log_file"
  echo ""

  if [[ "$AGENT" == "claude" ]]; then
    $AGENT_CMD "$(cat "$prompt_file")" 2>&1 | tee "$log_file"
  else
    cat "$prompt_file" | $AGENT_CMD 2>&1 | tee "$log_file"
  fi
}

# ============================================================================
# Append Progress Notes
# ============================================================================

append_progress() {
  cat >> "$PROGRESS_FILE" <<EOF

---

### [$1] $2

**Status**: $3
**Completed**: $(date '+%Y-%m-%d %H:%M:%S')
**Duration**: ${4}s
**Agent**: $AGENT

EOF
}

# ============================================================================
# Main Loop
# ============================================================================

echo "╔═══════════════════════════════════════════════════════════════╗"
echo "║ TASK LOOP                                                     ║"
echo "╠═══════════════════════════════════════════════════════════════╣"
printf "║ %-61s ║\n" "Agent: $AGENT"
printf "║ %-61s ║\n" "Max Iterations: $MAX_ITERATIONS"
printf "║ %-61s ║\n" "Dry Run: $DRY_RUN"
echo "╚═══════════════════════════════════════════════════════════════╝"

RUN_TAG="$(date +%Y%m%d-%H%M%S)-$$"

# Initial state update
[[ "$NO_MONITOR" != "true" ]] && update_state_json "starting" "" ""

# Track last completed task for state updates
LAST_COMPLETED_JSON=""

for i in $(seq 1 "$MAX_ITERATIONS"); do
  echo ""
  echo "═══════════════════════════════════════════════════════════════"
  echo "  Iteration $i of $MAX_ITERATIONS"
  echo "═══════════════════════════════════════════════════════════════"

  TASK_INFO=$(select_next_task)
  REMAINING=$(echo "$TASK_INFO" | python3 -c "import sys,json; print(json.load(sys.stdin).get('remaining',0))")
  COMPLETED=$(echo "$TASK_INFO" | python3 -c "import sys,json; print(json.load(sys.stdin).get('completed',0))")
  TOTAL=$(echo "$TASK_INFO" | python3 -c "import sys,json; print(json.load(sys.stdin).get('total',0))")
  TASK_JSON=$(echo "$TASK_INFO" | python3 -c "import sys,json; t=json.load(sys.stdin).get('task'); print(json.dumps(t) if t else '')")

  echo "Progress: $COMPLETED/$TOTAL completed, $REMAINING remaining"

  if [[ -z "$TASK_JSON" || "$TASK_JSON" == "null" ]]; then
    if [[ "$REMAINING" == "0" ]]; then
      [[ "$NO_MONITOR" != "true" ]] && update_state_json "completed" "" "$LAST_COMPLETED_JSON"
      echo -e "\n✓ ALL TASKS COMPLETED!"
      exit 0
    fi
    [[ "$NO_MONITOR" != "true" ]] && update_state_json "paused" "" "$LAST_COMPLETED_JSON"
    echo -e "\nNo actionable tasks. Check .claude/tasks.json for blocked tasks."
    exit 0
  fi

  TASK_ID=$(echo "$TASK_JSON" | python3 -c "import sys,json; print(json.load(sys.stdin).get('id',''))")
  TASK_NAME=$(echo "$TASK_JSON" | python3 -c "import sys,json; print(json.load(sys.stdin).get('name',''))")
  echo -e "\nTask: [$TASK_ID] $TASK_NAME\n"

  if [[ "$VERIFY_ONLY" == "true" ]]; then
    echo "Verifying success criteria..."
    set +e; verify_success_criteria "$TASK_JSON"; VERIFY_STATUS=$?; set -e
    [[ $VERIFY_STATUS -eq 0 ]] && echo -e "\n✓ Marking $TASK_ID complete" && update_task_status "$TASK_ID" "completed"
    continue
  fi

  if [[ "$DRY_RUN" == "true" ]]; then
    generate_prompt "$TASK_JSON" "$PROMPTS_DIR/$TASK_ID.md"
    echo "[DRY RUN] Would execute: $AGENT_CMD"
    continue
  fi

  # Track attempts for this task (Ralph-style retry)
  CURRENT_ATTEMPT=$(($(get_task_attempts "$TASK_ID") + 1))
  set_task_attempts "$TASK_ID" "$CURRENT_ATTEMPT"

  update_task_status "$TASK_ID" "in_progress"

  # Add attempt number to task JSON for state tracking
  TASK_JSON_WITH_ATTEMPT=$(echo "$TASK_JSON" | python3 -c "import sys,json; d=json.load(sys.stdin); d['attempt']=$CURRENT_ATTEMPT; print(json.dumps(d))")
  [[ "$NO_MONITOR" != "true" ]] && update_state_json "running" "$TASK_JSON_WITH_ATTEMPT" "$LAST_COMPLETED_JSON"

  # Generate prompt (with failure context if retry)
  LOG_FILE="$RUNS_DIR/run-$RUN_TAG-$TASK_ID-attempt$CURRENT_ATTEMPT.log"
  PREV_LOG="$(get_task_last_log "$TASK_ID")"

  if [[ $CURRENT_ATTEMPT -gt 1 && -n "$PREV_LOG" ]]; then
    echo "Retry attempt $CURRENT_ATTEMPT (Ralph-style with failure context)"
    FAILED_CRITERIA="$(get_task_failed_criteria "$TASK_ID")"
    [[ -z "$FAILED_CRITERIA" ]] && FAILED_CRITERIA="Verification failed"
    generate_prompt_with_failure "$TASK_JSON" "$PROMPTS_DIR/$TASK_ID.md" "$PREV_LOG" "$CURRENT_ATTEMPT" "$FAILED_CRITERIA"
  else
    generate_prompt "$TASK_JSON" "$PROMPTS_DIR/$TASK_ID.md"
  fi

  ITER_START=$(date +%s)
  set +e; run_agent "$PROMPTS_DIR/$TASK_ID.md" "$LOG_FILE"; CMD_STATUS=$?; set -e
  ITER_DURATION=$(($(date +%s) - ITER_START))

  # Store log path for potential retry
  set_task_last_log "$TASK_ID" "$LOG_FILE"

  echo -e "\nAgent finished (exit: $CMD_STATUS, duration: ${ITER_DURATION}s, attempt: $CURRENT_ATTEMPT)\n"

  [[ $CMD_STATUS -eq 130 || $CMD_STATUS -eq 143 ]] && update_task_status "$TASK_ID" "pending" "Interrupted" && exit $CMD_STATUS

  echo "Verifying success criteria..."
  # Capture verification output for retry context
  VERIFY_OUTPUT=$(verify_success_criteria "$TASK_JSON" 2>&1) || true
  echo "$VERIFY_OUTPUT"
  echo "$VERIFY_OUTPUT" | grep -q "^PASS:" && VERIFY_STATUS=0 || VERIFY_STATUS=1

  if [[ $VERIFY_STATUS -eq 0 ]]; then
    echo -e "\n✓ Task completed: $TASK_ID (attempt $CURRENT_ATTEMPT)"
    update_task_status "$TASK_ID" "completed"
    update_context "$TASK_ID" "$TASK_NAME" "$((COMPLETED + 1))" "$TOTAL"
    append_progress "$TASK_ID" "$TASK_NAME" "Completed ✓ (attempt $CURRENT_ATTEMPT)" "$ITER_DURATION"
    # Reset attempts on success
    clear_task_retry_state "$TASK_ID"
    # Store last completed for state updates
    LAST_COMPLETED_JSON="$TASK_JSON"
    [[ "$NO_MONITOR" != "true" ]] && update_state_json "running" "" "$LAST_COMPLETED_JSON"
  else
    # Store failed criteria for retry prompt
    set_task_failed_criteria "$TASK_ID" "$VERIFY_OUTPUT"

    if [[ $CURRENT_ATTEMPT -ge $MAX_TASK_RETRIES ]]; then
      echo -e "\n✗ Task failed after $MAX_TASK_RETRIES attempts. Marking as BLOCKED."
      update_task_status "$TASK_ID" "blocked" "Failed verification after $MAX_TASK_RETRIES attempts"
      append_to_blockers "$TASK_ID" "$TASK_NAME" "$VERIFY_OUTPUT" "$MAX_TASK_RETRIES"
      append_progress "$TASK_ID" "$TASK_NAME" "BLOCKED (failed $MAX_TASK_RETRIES attempts)" "$ITER_DURATION"
      # Reset attempts
      clear_task_retry_state "$TASK_ID"
    else
      echo -e "\n✗ Attempt $CURRENT_ATTEMPT failed. Will retry with failure context (Ralph-style)."
      # Keep status as in_progress for immediate retry
      append_progress "$TASK_ID" "$TASK_NAME" "Attempt $CURRENT_ATTEMPT failed, retrying" "$ITER_DURATION"
    fi
    [[ "$NO_MONITOR" != "true" ]] && update_state_json "running" "$TASK_JSON" "$LAST_COMPLETED_JSON"
  fi

  sleep 1
done

[[ "$NO_MONITOR" != "true" ]] && update_state_json "paused" "" "$LAST_COMPLETED_JSON"
echo -e "\nReached max iterations ($MAX_ITERATIONS)."
```

---

## Step 3: Write the Script

After getting the agent selection, write the script to the project root:

1. Substitute `${AGENT_DEFAULT}` with the selected agent (`codex` or `claude`)
2. Write to `./task-loop.sh`
3. Make executable: `chmod +x task-loop.sh`

```bash
# After writing the script
chmod +x task-loop.sh
```

---

## Step 4: Explain Usage

After generating the script, provide this usage guide:

```
✓ Generated task-loop.sh

Usage:
  ./task-loop.sh                    # Run all pending tasks
  ./task-loop.sh --agent claude     # Use Claude instead of Codex
  ./task-loop.sh --max 5            # Run max 5 tasks
  ./task-loop.sh --dry-run          # Preview prompts without executing
  ./task-loop.sh --verify-only      # Just verify criteria, mark complete if passing
  ./task-loop.sh --no-monitor       # Disable HTML monitor (for CI/headless)

The script will:
1. Open the HTML monitor in your browser (unless --no-monitor)
2. Find the next pending task with dependencies met
3. Generate a prompt with task details and context
4. Run the agent (codex/claude)
5. Verify success criteria automatically
6. Update .claude/ state files
7. Continue to next task

Monitor: http://localhost:8111/monitor.html
- Real-time task progress visualization
- Current task details and log output
- Polls state.json every 2 seconds

Logs are saved to: .claude/runs/
Prompts are saved to: .claude/prompts/

To monitor progress:
  Open http://localhost:8111/monitor.html  # Visual dashboard
  tail -f .claude/runs/run-*.log           # Watch current task
  cat .claude/PROGRESS-NOTES.md            # View completed tasks
  cat .claude/CONTEXT.md                   # View current state
```

---

## How It Works

The generated script:

1. **Starts monitor** — Copies monitor.html, starts HTTP server on port 8111, opens browser
2. **Selects tasks** — Finds first `pending` task with all `depends_on` tasks `completed`
3. **Generates prompts** — Builds prompt from task details, CONTEXT.md, ARCHITECTURE.md
4. **Runs agent** — Pipes prompt to codex/claude CLI
5. **Verifies criteria** — Runs each `success_criteria` check (file_exists, file_contains, command_succeeds, etc.)
6. **Retries with context** — If verification fails, retries immediately with failure context (Ralph-style)
7. **Updates state** — Marks task complete, updates state.json, CONTEXT.md, PROGRESS-NOTES.md
8. **Loops** — Continues until all tasks done or max iterations reached

**Ralph-Style Retry Behavior**:
- Each task gets up to 3 attempts (configurable via `MAX_TASK_RETRIES`)
- On failure, the task is retried IMMEDIATELY (not deferred)
- Retry prompts include:
  - Previous attempt's log output (last 100 lines)
  - Which success criteria failed
  - Instructions to analyze and fix the failure
- After 3 failures, task is marked "blocked" and documented in BLOCKERS.md
- This follows the "Ralph" pattern: failures are learning opportunities, not dead ends

**Monitor features**:
- Real-time HTML dashboard at `http://localhost:8111/monitor.html`
- Polls `state.json` every 2 seconds for live updates
- Shows tasks as visual blocks with status colors
- Displays current task details, attempt number, and log tail
- Auto-cleans up server on script exit

**Key difference from manual subagent spawning**:
- No completion signals to parse — verification is done via `success_criteria`
- Agent updates .claude/ files as part of its task (instructed in prompt)
- Script validates and updates as fallback
- Failures feed back into retry attempts with full context

---

## Success Criteria Types

The script supports these criteria types in `tasks.json`:

| Type | What it checks |
|------|----------------|
| `file_exists` | Target file/directory exists |
| `file_contains` | File contains regex pattern |
| `command_succeeds` | Command returns exit 0 |
| `type_checks` | TypeScript compiles (default: `npx tsc --noEmit`) |
| `test_passes` | Tests pass (uses `command` field) |
| `lint_passes` | Linting passes (uses `command` field) |
| `server_responds` | HTTP endpoint returns expected status |
| `env_var_set` | Environment variable is defined and non-empty |
| `manual_verify` | Requires human confirmation (logged as warning, doesn't fail) |

---

## Troubleshooting

**Task keeps failing criteria:**
- Check the log file in `.claude/runs/`
- Run `./task-loop.sh --verify-only` to see which criteria fail
- The agent may need manual intervention

**Agent not found:**
```bash
# Install codex
npm i -g @openai/codex

# Install claude
npm i -g @anthropic-ai/claude-code
```

**All tasks blocked:**
- Check `.claude/tasks.json` for dependency chains
- Some tasks may have failed dependencies
- Use `--verify-only` to mark manually-completed tasks as done
